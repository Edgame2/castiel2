# ML training scripts (Plan §5.6; BI_SALES_RISK_TRAINING_SCRIPTS_SPEC)

- **train_risk_scoring.py** – risk-scoring-global / risk-scoring-{industry}. Parquet with REQUIRED + target_risk; XGBRegressor; save model.json. **RMSE, MAE, R2 logged to Azure ML run** when executed as Azure ML Job (Plan §874; requires scikit-learn, azureml-core in conda-risk-scoring-train). Azure ML register when AZURE_ML_* set (optional: pip install azure-ai-ml azure-identity). With **--deploy** and AZURE_ML_*: creates ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using score_risk_scoring.py and **conda-risk-scoring.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT.
- **score_risk_scoring.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of risk-scoring (Plan §4.1, §874). init/run; loads model.json (XGBoost Booster); request `{"input": [{"feature": v, ...}]}`, response `{"riskScore": float}`. Use as `code_configuration.scoring_script` when deploying. FEATURE_ORDER matches train_risk_scoring (minus target_risk).
- **conda-risk-scoring.yaml** – Conda env for risk-scoring deployment (train_risk_scoring.py --deploy): python, numpy, xgboost.
- **train_win_probability.py** – win-probability-model. Parquet with same feature set + target_win (0/1); filter is_closed==1; XGBClassifier + CalibratedClassifierCV (isotonic/sigmoid); Brier, AUC-ROC; saves model.joblib. **Brier, AUC_ROC logged to Azure ML run** when executed as Azure ML Job (TRAINING_SCRIPTS_SPEC §3.2; Plan §876; requires azureml-core in conda-win-probability-train). Azure ML register when AZURE_ML_* set. With **--deploy** and AZURE_ML_*: ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using **score_win_probability.py** and **conda-win-probability.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT. Requires scikit-learn.
- **score_win_probability.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of win-probability (Plan §4.1, §876). init/run; loads model.joblib (CalibratedClassifierCV + feature_columns); request `{"input": [{"feature": v, ...}]}`, response `{"winProbability": float, "confidence": float}`. Feature order from artifact.
- **conda-win-probability.yaml** – Conda env for win-probability deployment (train_win_probability.py --deploy): python, numpy, scikit-learn, xgboost.
- **train_lstm_trajectory.py** – risk-trajectory-lstm. Parquet with opportunityId, snapshotDate, risk_score, activity_count_30d, days_since_last_activity, target_risk_30/60/90 (or target_risk); Keras LSTM; saves risk_trajectory_lstm/saved_model. **loss, val_loss, mae, val_mae logged to Azure ML run** when executed as Azure ML Job (Plan §875; requires azureml-core in conda-lstm-train). Azure ML register when AZURE_ML_* set. With **--deploy** and AZURE_ML_*: ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using **score_lstm_trajectory.py** and **conda-lstm.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT; SEQUENCE_LENGTH for scoring. Requires tensorflow.
- **score_lstm_trajectory.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of risk-trajectory-lstm (Plan §4.1, §875). init/run; loads TF SavedModel; request `{"sequence": [[r,a,d],...]}` or `{"input": [{"sequence":...}]}`, response `{"risk_30", "risk_60", "risk_90", "confidence"}`. SEQUENCE_LENGTH env (default 30).
- **conda-lstm.yaml** – Conda env for LSTM deployment (train_lstm_trajectory.py --deploy): python, numpy, tensorflow.
- **train_prophet_forecast.py** – revenue-forecasting-model. Parquet with (date, revenue) or (ds, y) or (date, pipeline_value); aggregate by date; Prophet for P10/P50/P90 (interval_width). Saves model.joblib. **MAPE, RMSE, MAE (in-sample) logged to Azure ML run** when executed as Azure ML Job (Plan §877; requires azureml-core in conda-prophet-train). Azure ML register when AZURE_ML_* set. With **--deploy** and AZURE_ML_*: ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using **score_prophet_forecast.py** and **conda-prophet.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT. Requires prophet.
- **score_prophet_forecast.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of revenue-forecasting (Plan §4.1, §877). init/run; loads model.joblib (Prophet + interval_width, periods_default); request `{"history": [[ds,y],...] or [{"ds", "y"}], "periods": int (optional)}` or `{"input": [{"history": [...], "periods":...}]}`, response `{"p10", "p50", "p90"}`. Extends from max(history ds) or model.make_future_dataframe.
- **conda-prophet.yaml** – Conda env for Prophet deployment (train_prophet_forecast.py --deploy): python, numpy, pandas, prophet, joblib.
- **train_anomaly_isolation_forest.py** – anomaly-detection-isolation-forest (Phase 2). Parquet with feature columns only; optional is_anomaly for validation. sklearn.ensemble.IsolationForest; --param-contamination, --param-n_estimators. Saves model.joblib. **F1 (anomaly) logged to Azure ML run** when is_anomaly column exists and script runs as Azure ML Job (TRAINING_SCRIPTS_SPEC §3.4; requires azureml-core in conda-anomaly-train). Azure ML register when AZURE_ML_* set. With **--deploy** and AZURE_ML_*: ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using **score_anomaly_isolation_forest.py** and **conda-anomaly.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT. Uses scikit-learn.
- **score_anomaly_isolation_forest.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of anomaly-detection (Plan §4.1; §3.4). init/run; loads model.joblib (IsolationForest + feature_columns); request `{"input": [{"feature": v, ...}]}`, response `{"isAnomaly": int (-1|1), "anomalyScore": float}`. Feature order from artifact.
- **conda-anomaly.yaml** – Conda env for anomaly deployment (train_anomaly_isolation_forest.py --deploy): python, numpy, scikit-learn.
- **generate_synthetic_opportunities.py** – Synthetic data for risk-scoring / win-probability when real data < 3k/5k (§3.6). Domain rules: amount log-normal, probability beta, stage categorical, target_risk = f(probability, days_since_activity). --output-path, --n-samples, --tenant-id, --seed, --pct-closed. Output: Parquet to ml_training/synthetic/risk_scoring/ or --output-path. Uses numpy, pandas, pyarrow.
- **prepare_training_data.py** – Training prep (DATA_LAKE_LAYOUT §2.4; TRAINING_SCRIPTS_SPEC §2.1). Joins /risk_evaluations + /ml_outcomes and writes /ml_training/{model_id}/. --risk-evaluations-path, --ml-outcomes-path, --output-path, --model-id (outcome_joined | risk_scoring | win_probability), --tenant-id, --partition-date. outcome_joined = base + target_win, target_risk, is_closed for downstream enrichment; risk_scoring | win_probability add placeholder features so train_risk_scoring / train_win_probability can consume (for production, enrich with shard-manager or buildFeatureVector first). Deps: pandas, pyarrow; adlfs for abfs://. **conda-prepare-train.yaml** – Conda for prepare_training_data: pandas, pyarrow, adlfs. **azml-job-prepare-training.yaml** – Azure ML v2 command job; inputs risk_evaluations (uri_folder), ml_outcomes (uri_folder), model_id, tenant_id, partition_date; outputs training_prep (uri_folder). Chain before train_risk_scoring or train_win_probability in a pipeline. **azml-pipeline-prepare-then-risk-scoring.yaml** – Pipeline: prepare_training_data → train_risk_scoring; pipeline inputs risk_evaluations, ml_outcomes, model_name, industry_id, deploy_flag; prepare step outputs training_prep → train step inputs.training_data. **azml-pipeline-prepare-then-win-probability.yaml** – Pipeline: prepare (model_id=win_probability) → train_win_probability; inputs risk_evaluations, ml_outcomes, model_name, deploy_flag.
- **azml-job-risk-scoring.yaml** – Azure ML v2 command job for `train_risk_scoring.py` (Plan §874). Submit from `containers/ml-service/scripts`: `az ml job create --file azml-job-risk-scoring.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Set `inputs.deploy_flag.value=--deploy` to deploy after register. **Synthetic when real data < 3k:** run `generate_synthetic_opportunities.py --output-path /path/to.parquet` then use that path (or upload to Data Lake) as `inputs.training_data.path`. **From risk_evaluations+ml_outcomes:** run `az ml job create --file azml-job-prepare-training.yaml --set inputs.risk_evaluations.path=abfs://... --set inputs.ml_outcomes.path=abfs://...`; use `outputs.training_prep` as `inputs.training_data` in a pipeline or follow-on train job.
- **conda-risk-scoring-train.yaml** – Conda env for the risk-scoring Azure ML training job: python, pandas, pyarrow, xgboost, scikit-learn, azure-ai-ml, azure-identity, azureml-core (for run.log of RMSE, MAE, R2). Deployment uses conda-risk-scoring.yaml (lighter).
- **azml-job-lstm-trajectory.yaml** – Azure ML v2 command job for `train_lstm_trajectory.py` (Plan §875). Submit from `scripts`: `az ml job create --file azml-job-lstm-trajectory.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Input Parquet: opportunityId, snapshotDate, risk_score, activity_count_30d, days_since_last_activity, target_risk_30/60/90 (or target_risk); from risk_snapshots export or Data Lake. Set `inputs.deploy_flag.value=--deploy` to deploy. For scoring, SEQUENCE_LENGTH env (default 30).
- **conda-lstm-train.yaml** – Conda env for the LSTM Azure ML training job: python, pandas, pyarrow, tensorflow, azure-ai-ml, azure-identity, azureml-core (for run.log of loss, val_loss, mae, val_mae). Deployment uses conda-lstm.yaml (lighter).
- **azml-job-win-probability.yaml** – Azure ML v2 command job for `train_win_probability.py` (Plan §876). Submit from `scripts`: `az ml job create --file azml-job-win-probability.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Input Parquet: amount, probability, days_to_close, stage_encoded, industry_encoded, days_since_last_activity, activity_count_30d, stakeholder_count, target_win; is_closed (filter ==1). Source: /ml_training/win_probability/, risk_evaluations+outcomes, or `generate_synthetic_opportunities.py --pct-closed`. Set `inputs.deploy_flag.value=--deploy` to deploy. Deployment uses conda-win-probability.yaml.
- **conda-win-probability-train.yaml** – Conda env for the win-probability Azure ML training job: python, pandas, pyarrow, scikit-learn, xgboost, azure-ai-ml, azure-identity, azureml-core (for run.log of Brier, AUC_ROC). Deployment uses conda-win-probability.yaml (lighter).
- **azml-job-prophet.yaml** – Azure ML v2 command job for `train_prophet_forecast.py` (Plan §877). Submit from `scripts`: `az ml job create --file azml-job-prophet.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Input Parquet: (date, revenue) or (ds, y) or (date, pipeline_value) or (date, target_revenue); aggregate by date. Source: /ml_training/revenue_forecasting/, historical closed-won or pipeline by date. Set `inputs.deploy_flag.value=--deploy` to deploy. Deployment uses conda-prophet.yaml.
- **conda-prophet-train.yaml** – Conda env for the revenue-forecasting Prophet Azure ML training job: python, pandas, pyarrow, prophet, joblib, azure-ai-ml, azure-identity, azureml-core (for run.log of MAPE, RMSE, MAE). Deployment uses conda-prophet.yaml (lighter).
- **azml-job-anomaly.yaml** – Azure ML v2 command job for `train_anomaly_isolation_forest.py` (Plan Phase 2, TRAINING_SCRIPTS_SPEC §3.4). Submit from `scripts`: `az ml job create --file azml-job-anomaly.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Input Parquet: feature columns only; optional is_anomaly for validation. Source: /ml_training/anomaly/ or /ml_inference_logs/. Set `inputs.deploy_flag.value=--deploy` to deploy. Inputs: param_contamination (default 0.1), param_n_estimators (default 100).
- **conda-anomaly-train.yaml** – Conda env for the anomaly Azure ML training job: python, pandas, pyarrow, scikit-learn, joblib, azure-ai-ml, azure-identity, azureml-core (for run.log of F1 when is_anomaly present). Deployment uses conda-anomaly.yaml (lighter).
- Run as Azure ML Job or: `pip install -r requirements-training.txt` then `python train_risk_scoring.py --input-path /path/to.parquet`, `python train_win_probability.py --input-path /path/to.parquet`, `python train_lstm_trajectory.py --input-path /path/to.parquet`, `python train_prophet_forecast.py --input-path /path/to.parquet`, `python train_anomaly_isolation_forest.py --input-path /path/to.parquet`, `python generate_synthetic_opportunities.py --output-path /path/to.parquet`, or `python prepare_training_data.py --risk-evaluations-path /path/to/risk_evaluations --ml-outcomes-path /path/to/ml_outcomes --output-path /path/to/ml_training/risk_scoring/out.parquet --model-id risk_scoring`.
- Input columns, targets, and templates: [BI_SALES_RISK_TRAINING_SCRIPTS_SPEC](../../documentation/requirements/BI_SALES_RISK_TRAINING_SCRIPTS_SPEC.md).
