{
  "alerts": [
    {
      "name": "High API Error Rate",
      "description": "Alert when API error rate exceeds 5%",
      "condition": "rate(api_requests_total{status=~\"5..\"}[5m]) / rate(api_requests_total[5m]) * 100 > 5",
      "severity": "critical",
      "notificationChannels": ["email", "slack"],
      "runbook": "Check application logs, database connectivity, and external service status"
    },
    {
      "name": "Slow API Response Times",
      "description": "Alert when p95 response time exceeds 500ms",
      "condition": "histogram_quantile(0.95, rate(api_response_time_bucket[5m])) > 500",
      "severity": "warning",
      "notificationChannels": ["slack"],
      "runbook": "Check database query performance, cache hit rates, and external API latency"
    },
    {
      "name": "Critical API Response Times",
      "description": "Alert when p99 response time exceeds 1000ms",
      "condition": "histogram_quantile(0.99, rate(api_response_time_bucket[5m])) > 1000",
      "severity": "critical",
      "notificationChannels": ["email", "slack", "pagerduty"],
      "runbook": "Immediate investigation required. Check for database locks, slow queries, or service degradation"
    },
    {
      "name": "Redis Connection Failures",
      "description": "Alert when Redis connection failures occur",
      "condition": "rate(redis_connection_failures_total[5m]) > 0",
      "severity": "critical",
      "notificationChannels": ["email", "slack"],
      "runbook": "Check Redis service status, network connectivity, and authentication"
    },
    {
      "name": "Database Connection Failures",
      "description": "Alert when Cosmos DB connection failures occur",
      "condition": "rate(cosmos_connection_failures_total[5m]) > 0",
      "severity": "critical",
      "notificationChannels": ["email", "slack", "pagerduty"],
      "runbook": "Check Cosmos DB service status, connection strings, and firewall rules"
    },
    {
      "name": "Service Bus Queue Backlog",
      "description": "Alert when queue depth exceeds 100 pending messages",
      "condition": "service_bus_queue_depth > 100",
      "severity": "warning",
      "notificationChannels": ["slack"],
      "runbook": "Check worker processing rate, increase worker instances if needed"
    },
    {
      "name": "High Embedding Failure Rate",
      "description": "Alert when embedding failure rate exceeds 5%",
      "condition": "rate(embedding_jobs_failed_total[5m]) / rate(embedding_jobs_total[5m]) * 100 > 5",
      "severity": "warning",
      "notificationChannels": ["slack"],
      "runbook": "Check Azure OpenAI service status, rate limits, and embedding service logs"
    },
    {
      "name": "Slow Vector Search",
      "description": "Alert when vector search p95 latency exceeds 2 seconds",
      "condition": "histogram_quantile(0.95, rate(vector_search_duration_seconds_bucket[5m])) > 2",
      "severity": "warning",
      "notificationChannels": ["slack"],
      "runbook": "Check vector index configuration, cache hit rate, and query complexity"
    },
    {
      "name": "High Cost per Tenant",
      "description": "Alert when cost per tenant exceeds threshold",
      "condition": "cost_per_tenant > 100",
      "severity": "info",
      "notificationChannels": ["email"],
      "runbook": "Review tenant usage patterns and optimize resource consumption"
    },
    {
      "name": "Dead Letter Queue Accumulation",
      "description": "Alert when dead letter queue has messages",
      "condition": "service_bus_dlq_depth > 0",
      "severity": "warning",
      "notificationChannels": ["slack"],
      "runbook": "Investigate failed messages, check error logs, and reprocess if needed"
    },
    {
      "name": "Low Cache Hit Rate",
      "description": "Alert when cache hit rate drops below 80%",
      "condition": "rate(redis_cache_hits_total[5m]) / (rate(redis_cache_hits_total[5m]) + rate(redis_cache_misses_total[5m])) * 100 < 80",
      "severity": "info",
      "notificationChannels": ["slack"],
      "runbook": "Review cache TTL settings and warming strategies"
    },
    {
      "name": "Integration Sync Failures",
      "description": "Alert when integration sync failure rate exceeds 10%",
      "condition": "rate(sync_jobs_failed_total[5m]) / rate(sync_jobs_total[5m]) * 100 > 10",
      "severity": "warning",
      "notificationChannels": ["slack"],
      "runbook": "Check integration adapter logs, external API status, and authentication tokens"
    }
  ],
  "notificationChannels": {
    "email": {
      "type": "email",
      "recipients": ["ops@castiel.com"],
      "enabled": true
    },
    "slack": {
      "type": "slack",
      "webhookUrl": "${SLACK_WEBHOOK_URL}",
      "channel": "#alerts",
      "enabled": true
    },
    "pagerduty": {
      "type": "pagerduty",
      "integrationKey": "${PAGERDUTY_INTEGRATION_KEY}",
      "enabled": true
    }
  }
}







