# ML training scripts (Plan §5.6; BI_SALES_RISK_TRAINING_SCRIPTS_SPEC)

- **train_risk_scoring.py** – risk-scoring-global / risk-scoring-{industry}. Parquet with REQUIRED + target_risk; XGBRegressor; save model.json. Azure ML register when AZURE_ML_* set (optional: pip install azure-ai-ml azure-identity). With **--deploy** and AZURE_ML_*: creates ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using score_risk_scoring.py and **conda-risk-scoring.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT.
- **score_risk_scoring.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of risk-scoring (Plan §4.1, §874). init/run; loads model.json (XGBoost Booster); request `{"input": [{"feature": v, ...}]}`, response `{"riskScore": float}`. Use as `code_configuration.scoring_script` when deploying. FEATURE_ORDER matches train_risk_scoring (minus target_risk).
- **conda-risk-scoring.yaml** – Conda env for risk-scoring deployment (train_risk_scoring.py --deploy): python, numpy, xgboost.
- **train_win_probability.py** – win-probability-model. Parquet with same feature set + target_win (0/1); filter is_closed==1; XGBClassifier + CalibratedClassifierCV (isotonic/sigmoid); Brier, AUC-ROC; saves model.joblib. Azure ML register when AZURE_ML_* set. With **--deploy** and AZURE_ML_*: ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using **score_win_probability.py** and **conda-win-probability.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT. Requires scikit-learn.
- **score_win_probability.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of win-probability (Plan §4.1, §876). init/run; loads model.joblib (CalibratedClassifierCV + feature_columns); request `{"input": [{"feature": v, ...}]}`, response `{"winProbability": float, "confidence": float}`. Feature order from artifact.
- **conda-win-probability.yaml** – Conda env for win-probability deployment (train_win_probability.py --deploy): python, numpy, scikit-learn, xgboost.
- **train_lstm_trajectory.py** – risk-trajectory-lstm. Parquet with opportunityId, snapshotDate, risk_score, activity_count_30d, days_since_last_activity, target_risk_30/60/90 (or target_risk); Keras LSTM; saves risk_trajectory_lstm/saved_model. Azure ML register when AZURE_ML_* set. With **--deploy** and AZURE_ML_*: ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using **score_lstm_trajectory.py** and **conda-lstm.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT; SEQUENCE_LENGTH for scoring. Requires tensorflow.
- **score_lstm_trajectory.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of risk-trajectory-lstm (Plan §4.1, §875). init/run; loads TF SavedModel; request `{"sequence": [[r,a,d],...]}` or `{"input": [{"sequence":...}]}`, response `{"risk_30", "risk_60", "risk_90", "confidence"}`. SEQUENCE_LENGTH env (default 30).
- **conda-lstm.yaml** – Conda env for LSTM deployment (train_lstm_trajectory.py --deploy): python, numpy, tensorflow.
- **train_prophet_forecast.py** – revenue-forecasting-model. Parquet with (date, revenue) or (ds, y) or (date, pipeline_value); aggregate by date; Prophet for P10/P50/P90 (interval_width). Saves model.joblib. Azure ML register when AZURE_ML_* set. With **--deploy** and AZURE_ML_*: ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using **score_prophet_forecast.py** and **conda-prophet.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT. Requires prophet.
- **score_prophet_forecast.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of revenue-forecasting (Plan §4.1, §877). init/run; loads model.joblib (Prophet + interval_width, periods_default); request `{"history": [[ds,y],...] or [{"ds", "y"}], "periods": int (optional)}` or `{"input": [{"history": [...], "periods":...}]}`, response `{"p10", "p50", "p90"}`. Extends from max(history ds) or model.make_future_dataframe.
- **conda-prophet.yaml** – Conda env for Prophet deployment (train_prophet_forecast.py --deploy): python, numpy, pandas, prophet, joblib.
- **train_anomaly_isolation_forest.py** – anomaly-detection-isolation-forest (Phase 2). Parquet with feature columns only; optional is_anomaly for validation. sklearn.ensemble.IsolationForest; --param-contamination, --param-n_estimators. Saves model.joblib. Azure ML register when AZURE_ML_* set. With **--deploy** and AZURE_ML_*: ManagedOnlineEndpoint (model_name-ep), ManagedOnlineDeployment (blue) using **score_anomaly_isolation_forest.py** and **conda-anomaly.yaml**; env DEPLOY_INSTANCE_TYPE, DEPLOY_INSTANCE_COUNT. Uses scikit-learn.
- **score_anomaly_isolation_forest.py** – Scoring entrypoint for Azure ML ManagedOnlineDeployment of anomaly-detection (Plan §4.1; §3.4). init/run; loads model.joblib (IsolationForest + feature_columns); request `{"input": [{"feature": v, ...}]}`, response `{"isAnomaly": int (-1|1), "anomalyScore": float}`. Feature order from artifact.
- **conda-anomaly.yaml** – Conda env for anomaly deployment (train_anomaly_isolation_forest.py --deploy): python, numpy, scikit-learn.
- **generate_synthetic_opportunities.py** – Synthetic data for risk-scoring / win-probability when real data < 3k/5k (§3.6). Domain rules: amount log-normal, probability beta, stage categorical, target_risk = f(probability, days_since_activity). --output-path, --n-samples, --tenant-id, --seed, --pct-closed. Output: Parquet to ml_training/synthetic/risk_scoring/ or --output-path. Uses numpy, pandas, pyarrow.
- **azml-job-risk-scoring.yaml** – Azure ML v2 command job for `train_risk_scoring.py` (Plan §874). Submit from `containers/ml-service/scripts`: `az ml job create --file azml-job-risk-scoring.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Set `inputs.deploy_flag.value=--deploy` to deploy after register. **Synthetic when real data < 3k:** run `generate_synthetic_opportunities.py --output-path /path/to.parquet` then use that path (or upload to Data Lake) as `inputs.training_data.path`.
- **conda-risk-scoring-train.yaml** – Conda env for the risk-scoring Azure ML training job: python, pandas, pyarrow, xgboost, azure-ai-ml, azure-identity. Deployment uses conda-risk-scoring.yaml (lighter).
- **azml-job-lstm-trajectory.yaml** – Azure ML v2 command job for `train_lstm_trajectory.py` (Plan §875). Submit from `scripts`: `az ml job create --file azml-job-lstm-trajectory.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Input Parquet: opportunityId, snapshotDate, risk_score, activity_count_30d, days_since_last_activity, target_risk_30/60/90 (or target_risk); from risk_snapshots export or Data Lake. Set `inputs.deploy_flag.value=--deploy` to deploy. For scoring, SEQUENCE_LENGTH env (default 30).
- **conda-lstm-train.yaml** – Conda env for the LSTM Azure ML training job: python, pandas, pyarrow, tensorflow, azure-ai-ml, azure-identity. Deployment uses conda-lstm.yaml (lighter).
- **azml-job-win-probability.yaml** – Azure ML v2 command job for `train_win_probability.py` (Plan §876). Submit from `scripts`: `az ml job create --file azml-job-win-probability.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Input Parquet: amount, probability, days_to_close, stage_encoded, industry_encoded, days_since_last_activity, activity_count_30d, stakeholder_count, target_win; is_closed (filter ==1). Source: /ml_training/win_probability/, risk_evaluations+outcomes, or `generate_synthetic_opportunities.py --pct-closed`. Set `inputs.deploy_flag.value=--deploy` to deploy. Deployment uses conda-win-probability.yaml.
- **conda-win-probability-train.yaml** – Conda env for the win-probability Azure ML training job: python, pandas, pyarrow, scikit-learn, xgboost, azure-ai-ml, azure-identity. Deployment uses conda-win-probability.yaml (lighter).
- **azml-job-prophet.yaml** – Azure ML v2 command job for `train_prophet_forecast.py` (Plan §877). Submit from `scripts`: `az ml job create --file azml-job-prophet.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Input Parquet: (date, revenue) or (ds, y) or (date, pipeline_value) or (date, target_revenue); aggregate by date. Source: /ml_training/revenue_forecasting/, historical closed-won or pipeline by date. Set `inputs.deploy_flag.value=--deploy` to deploy. Deployment uses conda-prophet.yaml.
- **conda-prophet-train.yaml** – Conda env for the revenue-forecasting Prophet Azure ML training job: python, pandas, pyarrow, prophet, joblib, azure-ai-ml, azure-identity. Deployment uses conda-prophet.yaml (lighter).
- **azml-job-anomaly.yaml** – Azure ML v2 command job for `train_anomaly_isolation_forest.py` (Plan Phase 2, TRAINING_SCRIPTS_SPEC §3.4). Submit from `scripts`: `az ml job create --file azml-job-anomaly.yaml --set inputs.training_data.path=abfs://...` or `azureml:dataset:1`. Input Parquet: feature columns only; optional is_anomaly for validation. Source: /ml_training/anomaly/ or /ml_inference_logs/. Set `inputs.deploy_flag.value=--deploy` to deploy. Inputs: param_contamination (default 0.1), param_n_estimators (default 100).
- **conda-anomaly-train.yaml** – Conda env for the anomaly Azure ML training job: python, pandas, pyarrow, scikit-learn, joblib, azure-ai-ml, azure-identity. Deployment uses conda-anomaly.yaml (lighter).
- Run as Azure ML Job or: `pip install -r requirements-training.txt` then `python train_risk_scoring.py --input-path /path/to.parquet`, `python train_win_probability.py --input-path /path/to.parquet`, `python train_lstm_trajectory.py --input-path /path/to.parquet`, `python train_prophet_forecast.py --input-path /path/to.parquet`, `python train_anomaly_isolation_forest.py --input-path /path/to.parquet`, or `python generate_synthetic_opportunities.py --output-path /path/to.parquet` (or use default ml_training/synthetic/risk_scoring/).
- Input columns, targets, and templates: [BI_SALES_RISK_TRAINING_SCRIPTS_SPEC](../../documentation/requirements/BI_SALES_RISK_TRAINING_SCRIPTS_SPEC.md).
