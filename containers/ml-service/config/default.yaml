# ML Service Module Configuration

# Application Insights (Plan §8.5.1, FIRST_STEPS §1); init via src/instrumentation.ts
application_insights:
  connection_string: ${APPLICATIONINSIGHTS_CONNECTION_STRING:-}
  disable: ${APPLICATIONINSIGHTS_DISABLE:-false}

# Prometheus metrics (Plan §8.5.2, §8.5.4); deployment/monitoring/README
metrics:
  path: ${METRICS_PATH:-/metrics}
  require_auth: ${METRICS_REQUIRE_AUTH:-false}
  bearer_token: ${METRICS_BEARER_TOKEN:-}

module:
  name: ml-service
  version: 1.0.0

server:
  port: ${PORT:-3033}
  host: ${HOST:-0.0.0.0}

cosmos_db:
  endpoint: ${COSMOS_DB_ENDPOINT}
  key: ${COSMOS_DB_KEY}
  database_id: ${COSMOS_DB_DATABASE_ID:-castiel}
  containers:
    models: ml_models
    features: ml_features
    feature_snapshots: ml_feature_snapshots
    feature_metadata: ml_feature_metadata
    training_jobs: ml_training_jobs
    evaluations: ml_evaluations
    predictions: ml_predictions
    jobs: multimodal_jobs
    win_probability: ml_win_probability_predictions
    # W6 Layer 8 – Learning Loop
    drift_metrics: ml_drift_metrics
    improvement_opportunity: ml_improvement_opportunity
    # Super Admin §4.4.2 – ML monitoring alert rules (partitionKey: tenantId)
    alert_rules: ml_alert_rules

services:
  ai_service:
    url: ${AI_SERVICE_URL:-http://localhost:3006}
  embeddings:
    url: ${EMBEDDINGS_URL:-http://localhost:3005}
  logging:
    url: ${LOGGING_URL:-http://localhost:3014}
  adaptive_learning:
    url: ${ADAPTIVE_LEARNING_URL:-http://localhost:3032}
  shard_manager:
    url: ${SHARD_MANAGER_URL:-http://localhost:3002}
  risk_analytics:
    url: ${RISK_ANALYTICS_URL:-http://localhost:3048}
  risk_catalog:
    url: ${RISK_CATALOG_URL:-http://localhost:3047}

# Feature pipeline (BI_SALES_RISK_FEATURE_PIPELINE_SPEC §6): label encoding for buildVectorForOpportunity. Defaults in code if absent.
feature_pipeline: {}

# Feature flags for BI/risk (Plan §895, §8.2). Env: USE_WIN_PROBABILITY_ML, USE_RISK_SCORING_ML.
feature_flags:
  use_win_probability_ml: true
  use_risk_scoring_ml: true
  use_revenue_forecasting_ml: true
  persist_win_probability: true

# Legacy; feature_flags preferred (Plan §8.1).
features: {}

# Plan §940, §9.3: model-monitoring thresholds. deployment/monitoring/runbooks/model-monitoring.md
model_monitoring:
  brier_threshold: 0.2
  psi_threshold: 0.2
  mae_threshold: 0.2

# Super Admin §5.2.2: feature version policy (versioning strategy, backward compatibility, deprecation).
feature_version_policy:
  versioningStrategy: semantic
  backwardCompatibility:
    enforceCompatibility: true
    allowBreakingChanges: false
    requireMigrationGuide: true
  deprecationPolicy:
    deprecationNoticeDays: 30
    supportOldVersionsDays: 90
    autoMigrate: false

# Super Admin §5.3.2: default feature quality rules (missing rate, drift, outlier method).
feature_quality_rules:
  missingRateThreshold: 0.1
  driftThreshold: 0.2
  outlierMethod: zscore
  outlierNStd: 3

# Plan §978: ONNX/Redis when p95≥500ms; defaults off. deployment/monitoring/runbooks/performance-optimization.md
onnx:
  enabled: false
  model_path: ""

cache:
  redis:
    enabled: false
    url: ""
    ttl_seconds: 3600

# Azure ML (BI_SALES_RISK Plan §5.1, §5.4, §8.2). Env: AZURE_ML_WORKSPACE_NAME, AZURE_ML_RESOURCE_GROUP, AZURE_ML_SUBSCRIPTION_ID.
# Endpoints env: AZURE_ML_ENDPOINT_RISK_GLOBAL, AZURE_ML_ENDPOINT_RISK_INDUSTRY, AZURE_ML_ENDPOINT_LSTM, AZURE_ML_ENDPOINT_WIN_PROB,
# AZURE_ML_ENDPOINT_FORECAST, AZURE_ML_ENDPOINT_CLUSTERING, AZURE_ML_ENDPOINT_PROPAGATION, AZURE_ML_ENDPOINT_ANOMALY, AZURE_ML_ENDPOINT_MITIGATION.
azure_ml:
  workspace_name: ""
  resource_group: "castiel-ml-prod-rg"
  subscription_id: ""
  endpoints:
    risk_scoring_global: ""
    risk_scoring_industry: ""
    risk_trajectory_lstm: ""
    win_probability: ""
    revenue_forecasting: ""
    clustering: ""
    propagation: ""
    anomaly: ""
    mitigation_ranking: ""
    win-probability-model: ""
    risk-scoring-model: ""
  api_key: ""

# Data Lake (Plan §9.3, §11.3, model-monitoring runbook §5): read /ml_inference_logs for PSI when implemented.
# Writer: logging DataLakeCollector. ml-service reads same container/path for drift.
data_lake:
  connection_string: ${DATA_LAKE_CONNECTION_STRING:-}
  container: ${DATA_LAKE_CONTAINER:-risk}
  ml_inference_logs_prefix: ${DATA_LAKE_ML_INFERENCE_LOGS_PREFIX:-/ml_inference_logs}

rabbitmq:
  url: ${RABBITMQ_URL}
  exchange: coder_events
  queue: ml_service
