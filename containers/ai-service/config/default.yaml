# AI Service Module Configuration

module:
  name: ai-service
  version: 1.0.0

server:
  port: ${PORT:-3006}
  host: ${HOST:-0.0.0.0}

cosmos_db:
  endpoint: ${COSMOS_DB_ENDPOINT}
  key: ${COSMOS_DB_KEY}
  database_id: ${COSMOS_DB_DATABASE_ID:-castiel}
  containers:
    models: ai_models
    completions: ai_completions
    agents: ai_agents
    insights: ai_insights
    proactive_insights: ai_proactive_insights
    collaborative_insights: ai_collaborative_insights
    risk_analysis: ai_risk_analysis
    tasks: reasoning_tasks
    prompts: prompt_prompts
    versions: prompt_versions
    ab_tests: prompt_ab_tests
    analytics: prompt_analytics
    llm_outputs: llm_outputs

services:
  secret_management:
    url: ${SECRET_MANAGEMENT_URL:-http://localhost:3003}
  logging:
    url: ${LOGGING_URL:-http://localhost:3014}
  shard_manager:
    url: ${SHARD_MANAGER_URL:-http://localhost:3023}
  embeddings:
    url: ${EMBEDDINGS_URL:-http://localhost:3005}

rabbitmq:
  url: ${RABBITMQ_URL}
  exchange: coder_events
  queue: ai_service

redis:
  url: ${REDIS_URL}
  host: ${REDIS_HOST:-localhost}
  port: ${REDIS_PORT:-6379}
  password: ${REDIS_PASSWORD:-}
  db: ${REDIS_DB:-0}

# LLM provider (merged from llm-service; configurable per tenant later)
llm:
  provider: ${LLM_PROVIDER:-mock}
  model: ${LLM_MODEL:-gpt-4}
  timeout_ms: ${LLM_TIMEOUT_MS:-10000}
